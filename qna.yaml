version: 3
domain: neurotech
created_by: paulhb7
seed_examples:
  - context: |
      To handle the scarcity and heterogeneity of electroencephalography (EEG) data for Brain-Computer Interface (BCI) tasks, and to harness the power of large publicly available datasets, we propose a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale dataset using a self-supervised task that learns how to reconstruct masked EEG segments.
    questions_and_answers:
      - question: What is Neuro-GPT?
        answer: |
          Neuro-GPT is a foundation model consisting of an EEG encoder and a GPT model, pre-trained on EEG datasets to handle data scarcity and heterogeneity for BCI tasks.
      - question: What task does Neuro-GPT perform during pre-training?
        answer: |
          Neuro-GPT learns to reconstruct masked EEG segments as part of its self-supervised pre-training task.
      - question: Why was Neuro-GPT developed?
        answer: |
          Neuro-GPT was developed to address data scarcity and heterogeneity in EEG datasets for BCI tasks by leveraging large-scale pre-training.
  - context: |
      The foundation model includes an EEG encoder with convolutional and self-attention layers. The EEG encoder processes raw EEG signals by extracting spatio-temporal features. These features are represented as tokens, which are then passed to the GPT model. The GPT model applies causal masking to predict masked tokens in the EEG sequence.
    questions_and_answers:
      - question: How does Neuro-GPT process EEG data?
        answer: |
          Neuro-GPT processes EEG data using an EEG encoder with convolutional and self-attention layers to extract spatio-temporal features, which are passed as tokens to a GPT model.
      - question: What is causal masking in Neuro-GPT?
        answer: |
          Causal masking involves masking specific EEG sequence tokens and predicting them based on prior tokens in the sequence.
      - question: What role does the EEG encoder play in Neuro-GPT?
        answer: |
          The EEG encoder extracts meaningful spatio-temporal features from raw EEG signals for input into the GPT model.
  - context: |
      The pre-training of Neuro-GPT uses the Temple University Hospital (TUH) EEG dataset. The TUH dataset includes recordings from 14,987 subjects with various clinical EEG configurations. The recordings were preprocessed using bandpass filters, notch filters, and z-normalization, and re-sampled to 250 Hz.
    questions_and_answers:
      - question: What dataset is used to pre-train Neuro-GPT?
        answer: |
          Neuro-GPT is pre-trained on the Temple University Hospital (TUH) EEG dataset.
      - question: How was the TUH EEG data preprocessed?
        answer: |
          TUH EEG data was preprocessed using bandpass filters, notch filters, z-normalization, and resampled to 250 Hz.
      - question: How many subjects are included in the TUH EEG dataset?
        answer: |
          The TUH EEG dataset includes recordings from 14,987 subjects.
  - context: |
      The Brain Foundation Model (BFM) extends the principles of foundational models to brain data, including EEG and fMRI. It leverages transfer learning and probabilistic modeling to create robust representations of brain activity, generalizable across datasets and modalities.
    questions_and_answers:
      - question: What is the Brain Foundation Model (BFM)?
        answer: |
          The Brain Foundation Model (BFM) is a general-purpose framework that uses transfer learning and probabilistic modeling to analyze EEG and fMRI data for robust and scalable brain activity representations.
      - question: How does BFM differ from Neuro-GPT?
        answer: |
          While Neuro-GPT focuses on EEG data and self-supervised learning, BFM incorporates multiple modalities like fMRI and uses probabilistic modeling to handle time-series data.
      - question: What datasets are used in BFM?
        answer: |
          BFM uses datasets such as NMT-EEG and ABCD-fMRI to train and evaluate its performance across modalities.
  - context: |
      The MOABB benchmark provides an open-source platform for evaluating EEG-based BCI pipelines. It highlights the superior performance of Riemannian geometry-based approaches and identifies best practices for reproducible benchmarking.
    questions_and_answers:
      - question: What is the purpose of the MOABB benchmark?
        answer: |
          The MOABB benchmark standardizes the evaluation of EEG-based BCI pipelines, promoting reproducibility and providing insights into optimal methodologies.
      - question: Why are Riemannian approaches preferred in MOABB benchmarks?
        answer: |
          Riemannian approaches leverage covariance matrices' geometry, providing robust classification performance even with noisy or limited data.
      - question: What are the key findings of the MOABB benchmark?
        answer: |
          The MOABB benchmark highlights that Riemannian pipelines consistently outperform deep learning pipelines in terms of accuracy and computational efficiency.
document_outline: |
  Information about the BCI foundation model, deep learning and BCI models
document:
  repo: https://github.com/Paulhb7/instructlab_bci
  commit: 1308ff5f47547923fdfc532c626c16dc708b9dd2
  patterns:
    - /Users/paulb/.local/share/instructlab/taxonomy/knowledge/technology/brain_computer_interfaces/neurotech.md

# Ensure there is a newline here at the end of the file
